<!-- saved from url=(0052)https://richzhang.github.io/InteractiveColorization/ -->
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript" async="" src="./template_files/js"></script>
    <script async="" src="./template_files/analytics.js"></script>
    <script src="./template_files/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
        google.load("jquery", "1.3.2");
    </script>

    <style type="text/css">
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
            font-weight: 300;
            font-size: 18px;
            margin-left: auto;
            margin-right: auto;
            width: 1250px;
        }
        
        h1 {
            font-weight: 300;
        }
        
        .disclaimerbox {
            background-color: #eee;
            border: 1px solid #eeeeee;
            border-radius: 10px;
            -moz-border-radius: 10px;
            -webkit-border-radius: 10px;
            padding: 20px;
        }
        
        video.header-vid {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px;
            -moz-border-radius: 10px;
            -webkit-border-radius: 10px;
        }
        
        img.header-img {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px;
            -moz-border-radius: 10px;
            -webkit-border-radius: 10px;
        }
        
        img.rounded {
            border: 0px solid #eeeeee;
            border-radius: 10px;
            -moz-border-radius: 10px;
            -webkit-border-radius: 10px;
        }
        
        a:link,
        a:visited {
            color: #1367a7;
            text-decoration: none;
        }
        
        a:hover {
            color: #208799;
        }
        
        td.dl-link {
            height: 160px;
            text-align: center;
            font-size: 22px;
        }
        
        .layered-paper-big {
            /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
            /* The fifth layer shadow */
            margin-left: 10px;
            margin-right: 45px;
        }
        
        .layered-paper {
            /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
            /* The third layer shadow */
            margin-top: 5px;
            margin-left: 10px;
            margin-right: 30px;
            margin-bottom: 5px;
        }
        
        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }
        
        hr {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }
    </style>



    <title>CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with GPT-4V</title>
    <!-- <meta property="og:title" content="CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with GPT-4V"> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1185.0" data-gr-ext-installed="">
    <br>
    <center>
        <span style="font-size:42px">CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with GPT-4V</span><br>
        <table align="center" width="950px">
            <!-- <table align=center width=540px> -->
            <tbody>

                <tr>
                    <td align="center" width="140px">
                        <center>
                            <span style="font-size:24px"><a href="https://siyuxu.com">Siyu Xu</a></span>
                        </center>
                    </td>
                    <td align="center" width="140px">
                        <center>
                            <span style="font-size:24px"><a href="https://yunke-wang.github.io/">Yunke Wang</a></span>
                        </center>
                    </td>
                    <td align="center" width="140px">
                        <center>
                            <span style="font-size:24px"><a href="https://daochang.site/">Daochang Liu</a></span>
                        </center>
                    </td>
                    <!-- </tr> -->
                    <!-- </table> -->
                    <!-- <table align=center width=600px> -->
                    <!-- <tr> -->
                    <td align="center" width="140px">
                        <center>
                            <span style="font-size:24px"><a href="https://scholar.google.com/citations?hl=en&user=Shy1gnMAAAAJ">Bo Du</a></span>
                        </center>
                    </td>
                    <td align="center" width="140px">
                        <center>
                            <span style="font-size:24px"><a href="http://changxu.xyz/">Chang Xu</a></span>
                        </center>
                    </td>
                </tr>
            </tbody>
        </table>
        <!-- <span style="font-size:16px">(*indicates equal contribution)</span><br> -->
        <table align="center" width="900px">
            <tbody>
                <tr>
                    <td align="center" width="240px">
                        <center>
                            <span style="font-size:22px">Code <a href="https://github.com/siyuhsu/CollagePrompt"> [GitHub]</a> </span>
                        </center>
                    </td>
                    <td align="center" width="200px">
                        <center>
                            <span style="font-size:22px">Arxiv<a href="https://arxiv.org/abs/2403.11468"> [Paper]</a></span>
                        </center>
                    </td>
                    <td align="center" width="300px">
                        <center>
                            <span style="font-size:22px">Dataset<a href="https://drive.google.com/file/d/1UVK0GhE1aQm1Fq7JDx93oZ4xpD2ZCUT8/view?usp=drive_link"> [Google Drive]</a> <a href="https://www.kaggle.com/datasets/siyuxu/collageprompt">[Kaggle] </a></span>
                        </center>
                    </td>
                </tr>
            </tbody>
        </table>
    </center>


    <center>
        <h1>Abstract</h1>
    </center>
    <p>
        Recent advancements in generative AI have suggested that by taking visual prompts, GPT-4V can demonstrate significant proficiency in visual recognition tasks. Despite its impressive capabilities, the financial cost associated with GPT-4V's inference presents
        a substantial barrier to its wide use. To address this challenge, we propose a budget-friendly collage prompting task that collages multiple images into a single visual prompt and makes GPT-4V perform visual recognition on several images simultaneously,
        thereby reducing the cost. We collect a <i>dataset</i> of various collage prompts to assess its performance in GPT-4V's visual recognition. Our evaluations reveal several key findings:
        <b>1)</b> Recognition accuracy varies with different positions in the collage. <b>2)</b> Grouping images of the same category together leads to better visual recognition results. <b>3)</b> Incorrect labels often come from adjacent images. These
        findings highlight the importance of image arrangement within collage prompt. To this end, we construct a <i>benchmark</i> called <b>CollagePrompt</b>, which offers a platform for designing collage prompt to achieve more cost-effective visual
        recognition with GPT-4V. A <i>baseline</i> method derived from genetic algorithms to optimize collage layouts is proposed and two <i>metrics</i> are introduced to measure the efficiency of the optimized collage prompt. Our benchmark enables researchers
        to better optimize collage prompts, thus making GPT-4V more cost-effective in visual recognition.
    </p>
    <center>
        <h1>Overview</h1>
    </center>

    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td width="1100px">
                    <center>
                        <img class="rounded" src="./resources/teaser.jpg" height="400px">
                        <br>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>

    <br> The <strong>CollagePrompt</strong> is a benchmark platform designed to address the financial challenges associated with utilizing GPT-4V for visual recognition tasks. By leveraging grid collages of various sizes, this benchmark provides an efficient
    and cost-effective approach to visual recognition without significantly compromising accuracy. The dataset includes a variety of visual prompts that have been carefully curated to facilitate robust testing and optimization of AI models.
    <br>
    <center>
        <h3>Dataset Statistics:</h3>
    </center>
    <ul>
        <li><strong>2x2 Collages:</strong>
            <ul>
                <li>Training Set: 25,000 collages and 110,250 collage prompts from ImageNet-1K</li>
                <li>Validation Set: 12,500 collages from ImageNet-1K</li>
            </ul>
        </li>
        <li><strong>3x3 Collages:</strong>
            <ul>
                <li>Training Set: 11,111 collages and 102,646 collages prompts from ImageNet-1K</li>
                <li>Validation Set: 5,555 collages from ImageNet-1K</li>
            </ul>
        </li>
        <li><strong>Additional Validation Sets:</strong> The validation sets also include collages from other common image recognition datasets such as Aircraft, Caltech101, DTD, EuroSAT, Food101, OxfordFlowers, OxfordPets, StanfordCars, SUN397, and UCF101.</li>
    </ul>

    <center>
        <h3>Key Features:</h3>
    </center>
    <ul>
        <li><strong>Cost Efficiency:</strong> Transitioning from single images to grid collages (e.g., 2x2) significantly reduces inference costs and time while maintaining an acceptable level of accuracy.</li>
        <li><strong>Grid Size Optimization:</strong> The benchmark highlights the practical value of optimizing arrangements for 2x2 and 3x3 grids, balancing cost and accuracy effectively.</li>
        <li><strong>Baseline Algorithms:</strong> The dataset has been used to train and evaluate baseline algorithms for optimizing collage prompts, providing a solid foundation for further research and development.</li>
    </ul>
    <br> Using the CollagePrompt benchmark, researchers can optimize image arrangements to minimize accuracy loss and reduce costs associated with GPT-4V’s visual recognition tasks.
    <hr>
    <center>
        <h1>Arrangement Matters</h1>
    </center>

    The arrangement of images within a collage prompt significantly impacts the overall recognition accuracy of GPT-4V. For any given set of images forming a collage, there should exist one or more optimal arrangements that maximize overall recognition accuracy.
    Our findings indicate that different arrangements can yield varying levels of accuracy, with a quadrant-grid collage having 24 (4!) potential image arrangements and a nine-grid format exceeding 360,000 (9!) possibilities; The statistics demonstrates
    the effect of these arrangements, emphasizing the need for effective optimization to minimize accuracy loss.

    <br>

    <table align="center" width="1200px">
        <tbody>
            <tr>
                <td align="center" width="1200px">
                    <!-- <a href="./index_files/legacy_v4.jpg"><img class="rounded"  src = "./index_files/legacy_v4_small.jpg" width = "800px"></a><br> -->
                    <img class="rounded" src="./resources/variance.jpg" width="800px"><br>
                    <span style="font-size:16px"></span>
                </td>
            </tr>
            <!-- <tr> -->
        </tbody>
    </table>

    <br>
    <hr>
    <!-- 
    <center>
        <h1>Benchmark</h1>
    </center>


    <hr> -->

    <center>
        <h1>Observations</h1>
    </center>
    Different positions within the collage grid have varying accuracy rates in GPT-4V's visual recognition. As shown in the figure, the top-left position in both 2×2 and 3×3 grids tends to have the highest accuracy, with accuracy decreasing towards the center
    and bottom-left positions, which have the lowest accuracy. Accuracy then improves slightly for the last row. This pattern suggests potential model fatigue when processing central images in the collage, leading to lower accuracy that recovers as the
    model approaches the final row. Based on this observation, a natural idea to optimize the arrangement is to place ‘hard’ images into positions with higher accuracy while leaving ‘easy’ images to remaining positions.
    <table align="center" width="600px">
        <tbody>
            <tr>
                <td align="center" width="600px">
                    <img class="rounded" src="./resources/obs_1.png" width="800px"><br>
                    <!-- <a href="./index_files/imagenet_showcase.jpg"><img class="rounded"  src = "./index_files/imagenet_showcase_small.jpg" width = "800px"></a><br> -->
                    <span style="font-size:16px"></span>
                </td>
            </tr>
            <tr>
                <td align="center" width="600px">
                    <center>
                        <span style="font-size:28px"> Observation 1</span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>
    <br>
    <hr> We observed that in both 2×2 and 3×3 collages, placing images of the same class together significantly improves GPT-4V's overall recognition accuracy. Conversely, when the order is shuffled and images of the same class are not adjacent, the accuracy
    decreases. This improvement can be due to clustering images of the same class reducing the complexity of batch recognition for GPT-4V.


    <table align="center" width="600px">
        <tbody>
            <tr>
                <td align="center" width="600px">
                    <!-- <a href="./index_files/lab_all_figures45k.jpg"><img class="rounded" src = "./index_files/lab_all_figures45k_small.jpg" width = "800px"></a><br> -->
                    <img class="rounded" src="./resources/obs_2a.png" width="800px"><br>
                    <span style="font-size:16px"></span>
                </td>
            </tr>
            <tr>
                <td align="center" width="600px">
                    <center>
                        <span style="font-size:28px"> Observation 2</span>
                    </center>

                </td>
            </tr>
        </tbody>

    </table>
    <br>
    <hr> We analyzed the prediction errors in 2×2 and 3×3 collages and found that the incorrectly predicted labels often correspond to images in adjacent positions. This indicates that the model correctly identifies the images but outputs the predictions
    to the wrong locations due to localization inaccuracies. When the arrangement order is changed, the model outputs the correctly predicted labels to the correct positions.
    <table align="center" width="600px">
        <tbody>
            <tr>
                <td align="center" width="600px">
                    <!-- <a href="./index_files/lab_all_figures45k.jpg"><img class="rounded" src = "./index_files/lab_all_figures45k_small.jpg" width = "800px"></a><br> -->
                    <img class="rounded" src="./resources/obs_3.png" width="800px"><br>
                    <span style="font-size:16px"></span>
                </td>
            </tr>
            <tr>
                <td align="center" width="600px">
                    <center>
                        <span style="font-size:28px"> Observation 3</span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>
    <br>
    <!--   		  <table align=center width=600px>
  		  	<tr>
  	              <td align=center width=300px>
  					<center>
  						<span style="font-size:22px"><a href='http://colorization.eecs.berkeley.edu/siggraph/global_transfer/'>Histogram Transfer Results</a></span>
	  		  		</center>
	  		  	  </td>
	  		</tr>
		  </table>
 -->
    <hr>
    <center>
        <h1>Try the baseline</h1>
    </center>
    Our CollagePrompt benchmark provides a foundational method to optimize collage layouts for cost-effective visual recognition using GPT-4V. The baseline method leverages genetic algorithms to determine the most efficient image arrangements within collages.
    This approach ensures that the recognition accuracy remains high while reducing the cost associated with multiple individual image recognitions.


    <table align="center" width="900px">
        <tbody>
            <tr>
                <td align="center" width="900px">
                    <img class="round" style="width:900px" src="./resources/baseline_lcp.png">
                </td>

            </tr>
            <tr>
                <td align="center" width="600px">
                    <center>
                        <span style="font-size:28px"> Baseline</span>
                    </center>
                </td>

            </tr>
        </tbody>
    </table>

    Our baseline algorithm employs a genetic algorithm to optimize collage prompts. The genetic algorithm iteratively evolves the arrangements of images within the collage to improve the overall recognition accuracy while keeping the inference cost low. Key
    steps of the baseline algorithm include:

    <ul align="left" width="900px">
        <li><strong>Initialization:</strong> Generate an initial population of random collage arrangements.</li>
        <li><strong>Selection:</strong> Choose the best-performing arrangements based on recognition accuracy.</li>
        <li><strong>Crossover:</strong> Combine pairs of selected arrangements to create new ones.</li>
        <li><strong>Mutation:</strong> Introduce slight variations to some arrangements to explore new possibilities.</li>
        <li><strong>Evaluation:</strong> Assess the new arrangements using the GPT-4V API to measure their accuracy.</li>
    </ul>

    <table align="center" width="600px">
        <tbody>
            <tr>
                <td align="center" width="600px">
                    <!-- <a href="./index_files/lab_all_figures45k.jpg"><img class="rounded" src = "./index_files/lab_all_figures45k_small.jpg" width = "800px"></a><br> -->
                    <img class="rounded" src="./resources/results.png" width="800px"><br>
                    <span style="font-size:16px"></span>
                </td>
            </tr>
            <tr>
                <td align="center" width="600px">
                    <center>
                        <span style="font-size:28px"> Experiment Results</span>
                    </center>
                </td>
            </tr>
        </tbody>
    </table>
    <br>

    <p align="left" width="900px">
        This baseline serves as a starting point for further research and optimization, providing a clear pathway to enhancing the cost-efficiency of visual recognition tasks using GPT-4V. The experimental results demonstrate significant improvements in recognition
        accuracy when using optimized collage prompts compared to random arrangements.
    </p>
    <table align="center" width="800px">
        <tbody>
            <tr></tr>
            <!-- <tr><center> -->
            <!-- <span style="font-size:28px"><a>&nbsp;Model: [Prototxt] Weights: [Unrescaled] [Rescaled]</a></span> -->
            <!-- </center></tr> -->
        </tbody>
    </table>

    <br>
    <hr>


    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td width="400px">
                    <left>
                        <center>
                            <h1>Related and Concurrent Work</h1>
                        </center>
                        <br> Cheng, Z., Kasai, J., & Yu, T. <b>Batch prompting: Efficient inference with large language model apis.</b> In EMNLP, 2023. <a href="https://arxiv.org/pdf/2301.08721"> [PDF]</a><a href="https://github.com/xlang-ai/batch-prompting"> [Website]</a><br>

                        <br> Lin, J., Diesendruck, M., Du, L., & Abraham, R. <b>BatchPrompt: Accomplish more with less. </b> In ICLR, 2024.
                        <a href="https://arxiv.org/pdf/2309.00384"> [PDF]</a><br>

                        <br> Anonymous. <b>Tune-n-Batch: Fine-Tuning LLMs for Batch Prompting.</b> In ACL submission, 2024. <a href="https://openreview.net/attachment?id=DPCBUZqn1h&name=pdf"> [PDF]</a><br>

                        <br> Yue, M., Zhao, J., Zhang, M., Du, L., & Yao, Z. <b>Large language model cascades with mixture of thoughts representations for cost-efficient reasoning.</b> In ICLR, 2024. <a href="https://arxiv.org/pdf/2310.03094"> [PDF]</a>
                        <a href="https://github.com/MurongYue/LLM_MoT_cascade"> [Website]</a><br>

                        <br> Wu, W., Yao, H., Zhang, M., Song, Y., Ouyang, W., & Wang, J. <b>GPT4Vis: what can GPT-4 do for zero-shot visual recognition?.</b> In arXiv, 2023. <a href="https://arxiv.org/pdf/2311.15732">[PDF]</a> <a href="https://github.com/whwu95/GPT4Vis"> [Website]</a><br>


                        <br> Jiang, Y., Irvin, J., Wang, J. H., Chaudhry, M. A., Chen, J. H., & Ng, A. Y. <b>Many-Shot In-Context Learning in Multimodal Foundation Models.</b> In Arxiv, 2024. <a href="https://arxiv.org/pdf/2405.09798"> [PDF]</a>
                        <a href="https://github.com/stanfordmlgroup/ManyICL"> [Website]</a><br>

                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>

    <table align=center width=600px>
        <tr>
            <td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

    <table align="center" width="1100px">
        <tbody>
            <tr>
                <td width="400px">
                    <left>
                        <center>
                            <h1>Acknowledgements</h1>
                        </center>
                        This template was originally made by <a href="http://richzhang.github.io/"> Richard Zhang</a> for the <a href="https://richzhang.github.io/InteractiveColorization/">Colorization project</a>.
                    </left>
                </td>
            </tr>
        </tbody>
    </table>


    <br><br>

    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-75863369-3', 'auto');
        ga('send', 'pageview');
    </script>





    <iframe id="6w4Di6G7" frameborder="0" src="./template_files/translateSandbox.html" style="width: 0px; height: 0px; display: none;"></iframe></body>
<grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration>

</html>